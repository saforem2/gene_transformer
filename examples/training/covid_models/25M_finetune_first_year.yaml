wandb_active: true
wandb_project_name: codon_transformer
wandb_entity_name: null
wandb_model_tag: patric_25M_9-30-finetune_first_year_maulik
checkpoint_dir: 9-30_128nodes_first_year_finetune
# without loading the pt checkpoint, you'd train from scratch
load_pt_checkpoint: /lus/eagle/projects/CVD-Mol-AI/mzvyagin/gene_transformer/gene_transformer/patric25M/9-28_128nodes/model-epoch01-val_loss0.57-v2_bias_removed.pt
node_local_path: null
num_nodes: 128
compute_throughput: false
profiling_path: null
enable_perplexity: true
log_every_n_steps: 5
val_check_interval: null
limit_val_batches: 32
check_val_every_n_epoch: 1
checkpoint_every_n_train_steps: 500
checkpoint_every_n_epochs: null
tokenizer_file: ../../genslm/tokenizer_files/codon_wordlevel_69vocab.json
train_file: /path/to/data/first_year/first_year_train.h5
val_file: /path/to/data/first_year/first_year_val.h5
test_file: /path/to/data/first_year/first_year_val.h5
kmer_size: 3
small_subset: 0
enable_blast: false
blast_validation_file: blast_file.fasta
num_blast_seqs_per_gpu: 5
blast_exe_path: blastn
model_config_json: ../../genslm/architectures/neox/neox_25,290,752_10240_pos_embed.json
batch_size: 1
epochs: 100
block_size: 10240
accumulate_grad_batches: 1
learning_rate: 5.0e-05
precision: 16
warm_up_lr: null
lr_plateau: null
deepspeed_cfg_file: null
offload_parameters: false
offload_optimizer: false
num_test_seqs_per_gpu: 0
custom_seq_name: SyntheticSeq
num_data_workers: 4
prefetch_factor: 4
pin_memory: true
persistent_workers: true
deepspeed_flops_profile: false